# PoLLaMa-Adapted
Adapter layer of LLaMa 7B, finetuned  (badly) on Polish language dataset, trained according to [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/pdf/2303.16199.pdf)
& https://github.com/ZrrSkywalker/LLaMA-Adapter  
Works slightly above expectations (not that they were particularly high)  
Trained on automatically translated version of [Alpaca dataset](https://github.com/gururise/AlpacaDataCleaned)
